{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRSdenoiser: Multiple branch convolutional neural network with final residual step for baseline subtraction and denoising of SRS spectra\n",
    "# Minimal code example for training and testing the SRSdenoiser NN architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "olympic-fifty",
   "metadata": {},
   "source": [
    "### Load requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import array\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import moment \n",
    "from tensorflow.keras.callbacks import History\n",
    "import skimage\n",
    "from skimage import metrics as sm\n",
    "\n",
    "from scipy import signal\n",
    "from scipy import io\n",
    "\n",
    "\n",
    "from scipy.signal import spline_filter\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go \n",
    "\n",
    "from scipy.signal import find_peaks \n",
    "import itertools\n",
    "\n",
    "\n",
    "import wandb\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "import Models.customLossLib as cll\n",
    "import Models.metrics_2 as mm\n",
    "import Models.models as net\n",
    "\n",
    "\n",
    "\n",
    "tole=1.1  ### Set the tolerance for the PeakFinder defined in Models.metrics_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3704d3",
   "metadata": {},
   "source": [
    "### Define methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a19d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load dataset \n",
    "\n",
    "def data_load(pathToData,dataset, shuf=True, seed=1990, prep=1,ShowPlot=True):\n",
    "    ''' \n",
    "    Load and preprocess the datasets\n",
    "    Inputs:\n",
    "        pathToData: path to the file to load\n",
    "        dataset: name of the file to load\n",
    "        shuf: if True enables shuffling of the dataset\n",
    "        seed: seed of the random shuffling, set to 1990 to reproduce the training/test split used in the paper\n",
    "        prep: can be either 0 or 1, allows to choose between two preprocessing routines\n",
    "        ShowPlot: if True plot examples of raw and GT spectra from the loaded dataset and print the sizes of the output variables\n",
    "    Outputs:\n",
    "        normX: factor to normalize the test data before inference. It is calculate as the norm of the training data\n",
    "        nbin: number of points in the input spectra (default for the HN and LN datasets is 801)\n",
    "        X_test, X_train: test and train tf tensors for the raw data\n",
    "        Y_test, Y_train: test and train tf tensors for the GT\n",
    "        GT_freq, GT_freq_train, GT_freq_test: arrays containg the spectral positions of the Raman bands in the GT\n",
    "    '''    \n",
    "    \n",
    "    import numpy as np\n",
    "    ###Load Dataset\n",
    "    \n",
    "    Sn = np.loadtxt(pathToData+'Snoise_'+dataset+'.txt')\n",
    "    Sc = np.loadtxt(pathToData+'Sclean_'+dataset+'.txt')\n",
    "\n",
    "    Sc = Sc.transpose()\n",
    "    Sn = Sn.transpose()\n",
    "\n",
    "\n",
    "    nbin=Sc.shape[1]\n",
    "\n",
    "\n",
    "    ###Shuffling\n",
    "    shuf = True\n",
    "    if shuf:\n",
    "        rng = np.random.RandomState(1990)\n",
    "        shuffler = rng.permutation(len(Sn))\n",
    "        Sn = Sn[shuffler]\n",
    "        Sc = Sc[shuffler]\n",
    "\n",
    "\n",
    "    ###Preprocessing: use prep=1 for the same normalization used in the paper\n",
    "    if ShowPlot:\n",
    "        plt.figure(figsize=(20,6))\n",
    "        for idx in range(6):\n",
    "            plt.subplot(1,6,idx+1)\n",
    "            plt.plot(Sn[idx], '-r')\n",
    "            plt.plot(Sc[idx], '-b')\n",
    "        plt.tight_layout()  \n",
    "\n",
    "\n",
    "    ev = int(Sc.shape[0]*0.8)\n",
    "    X_train = Sn[:ev]\n",
    "    X_test = Sn[ev:]\n",
    "\n",
    "    if prep==0:\n",
    "\n",
    "        maxX = np.max((X_train),axis=-1)\n",
    "        minX = np.min((X_train),axis=-1)\n",
    "        for i in range(len(maxX)):\n",
    "            X_train[i,:]= (X_train[i,:]-minX[i]) / (maxX[i]-minX[i])\n",
    "\n",
    "        maxXt = np.max((X_test),axis=-1)\n",
    "        minXt = np.min((X_test),axis=-1)\n",
    "        for i in range(len(maxXt)):\n",
    "            X_test[i,:]= (X_test[i,:]-minXt[i]) / (maxXt[i]-minXt[i])\n",
    "\n",
    "    elif prep==1:\n",
    "        stX=np.std(X_train,axis=-1)\n",
    "        meanX=np.mean(X_train,axis=-1)\n",
    "        for i in range(len(stX)):\n",
    "            X_train[i,:]= (X_train[i,:]-meanX[i]) / stX[i] \n",
    "\n",
    "        stXt=np.std(X_test,axis=-1)\n",
    "        meanXt=np.mean(X_test,axis=-1)\n",
    "        for i in range(len(stXt)):\n",
    "            X_test[i,:]= (X_test[i,:]-meanXt[i]) / stXt[i]\n",
    "\n",
    "\n",
    "    normX = np.max(np.abs(X_train))\n",
    "\n",
    "    X_train /= normX\n",
    "    X_test /= normX\n",
    "\n",
    "    X_train = X_train.reshape((X_train.shape[0],X_train.shape[1],1))\n",
    "    X_test = X_test.reshape((X_test.shape[0],X_test.shape[1],1))\n",
    "\n",
    "    Y_train = Sc[:ev]\n",
    "    Y_test = Sc[ev:]\n",
    "\n",
    "\n",
    "    if prep==0:\n",
    "\n",
    "        for i in range(len(maxX)):\n",
    "            Y_train[i,:]= (Y_train[i,:]) / (maxX[i]-minX[i])\n",
    "\n",
    "        for i in range(len(maxXt)):\n",
    "            Y_test[i,:]= (Y_test[i,:]) / (maxXt[i]-minXt[i])\n",
    "\n",
    "    elif prep==1:\n",
    "\n",
    "        for i in range(len(stX)):\n",
    "            Y_train[i,:]= Y_train[i,:] / stX[i] \n",
    "\n",
    "        for i in range(len(stXt)):\n",
    "            Y_test[i,:]= Y_test[i,:] / stXt[i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Y_train /= normX\n",
    "    Y_test /= normX\n",
    "\n",
    "    Y_train = Y_train.reshape((Y_train.shape[0],Y_train.shape[1],1))\n",
    "    Y_test = Y_test.reshape((Y_test.shape[0],Y_test.shape[1],1))\n",
    "    \n",
    "    if ShowPlot:\n",
    "        print(X_train.shape)\n",
    "        print(Y_train.shape)\n",
    "        print(X_test.shape)\n",
    "        print(Y_test.shape)\n",
    "\n",
    "    print('Norm factor is '+str(normX)+ '\\nUse this value to normalize input data during test and inference phase')\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    ###Load GT frequencies\n",
    "    while True:\n",
    "        try:\n",
    "            str_name = pathToData+dataset+'_GTfrequencies.mat'\n",
    "\n",
    "            GT_freq = io.loadmat(str_name)\n",
    "\n",
    "            GT_freq = GT_freq[\"freq\"]\n",
    "            GT_freq = np.squeeze(GT_freq)\n",
    "            if shuf:\n",
    "                GT_freq = GT_freq[shuffler]\n",
    "\n",
    "            GT_freq_train = GT_freq[:ev]    \n",
    "            GT_freq_test = GT_freq[ev:]\n",
    "            break\n",
    "        except:\n",
    "            print(\"GT_freq not loaded...\")\n",
    "            GT_freq_train = np.array(np.zeros((len(X_train),2)) ) \n",
    "            GT_freq_test = np.array(np.zeros((len(X_test),2)) )\n",
    "            break\n",
    "    \n",
    "    \n",
    "    return normX, nbin, X_test,X_train,Y_test,Y_train,GT_freq, GT_freq_train, GT_freq_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b3c80e",
   "metadata": {},
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb24ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set to True to use WandB as a logger\n",
    "\n",
    "logger = False\n",
    "\n",
    "if logger:\n",
    "    ###Install wandb: uncomment if need to install\n",
    "    #!pip install wandb \n",
    "    \n",
    "    ###Set up logger\n",
    "    import wandb\n",
    "    from wandb.keras import WandbCallback\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5629916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Custom callback to log plots of test set and metrics at selected epochs during training\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, model, x_test, y_test,x_test_all, y_test_all, gt_freq_test):\n",
    "\n",
    "        self.model = model\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.x_test_all = x_test_all\n",
    "        self.y_test_all = y_test_all\n",
    "        self.gt_freq_test = gt_freq_test\n",
    "        \n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        if epoch%5==0:\n",
    "            y_pred = self.model.predict(self.x_test)\n",
    "            nbin=self.y_test.shape[1]\n",
    "            print(\"Logging metrics and plots of predicted test set...\\n\")\n",
    "            \n",
    "            str_id='Test set: gt,pred'\n",
    "            str_id2='Experimental data'\n",
    "            xs = [i for i in range(nbin)]\n",
    "            fig = make_subplots(rows=1, cols=self.y_test.shape[0])\n",
    "            \n",
    "            len_x = self.y_test.shape[0]\n",
    "            len_x_all= self.y_test_all.shape[0]\n",
    "            for jj in range(len_x):\n",
    "\n",
    "\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=xs, y=self.y_test[jj,:].reshape(nbin,),name = 'GT',line = dict(color = 'rgb(22, 96, 167)', width = 2, dash = 'dot')),\n",
    "                    row=1, col=jj+1\n",
    "                )\n",
    "\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=xs, y=y_pred[jj,:].reshape(nbin,),name = 'Predicted',line = dict(color = 'rgb(205, 12, 24)', width = 1)),\n",
    "                    row=1, col=jj+1\n",
    "                )\n",
    "\n",
    "                fig.update_layout(height=600, width=800)\n",
    "\n",
    "                wandb.log({str_id : fig}, commit=False)\n",
    "                \n",
    "\n",
    "                \n",
    "            #Monitor selected metrics\n",
    "            \n",
    "            Y=self.model.predict(self.x_test_all).reshape(len_x_all,nbin)\n",
    "            X=self.x_test_all.reshape(len_x_all,nbin)\n",
    "            GT=self.y_test_all.reshape(len_x_all,nbin)\n",
    "\n",
    "            metrics_NN=mm.compute_metrics2(Y,X,GT,self.gt_freq_test)\n",
    "            wandb.log({'ssim_mean':np.mean(metrics_NN['ssim']),'ssim_std':np.std(metrics_NN['ssim']),\n",
    "                      'precision_mean':np.mean(metrics_NN['precision']),'precision_std':np.std(metrics_NN['precision']),\n",
    "                      },commit=True)\n",
    "\n",
    "           "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "verbal-receipt",
   "metadata": {},
   "source": [
    "## Training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "### Select folder and filename of the dataset\n",
    "\n",
    "dataset='HighNoise'\n",
    "pathToData='Datasets/HighNoise/'\n",
    "\n",
    "normFactor, nbin, X_test,X_train,Y_test,Y_train, GT_freq, GT_freq_train, GT_freq_test = data_load(pathToData,dataset, shuf=True,seed=1990, prep=1,ShowPlot=1)\n",
    "\n",
    "# Define hyperparameters\n",
    "\n",
    "ker=[21]\n",
    "max_ker=[88]\n",
    "bn=False\n",
    "nlayers=6\n",
    "nDense=0\n",
    "nConv=4\n",
    "\n",
    "Nparam=10000\n",
    "\n",
    "\n",
    "# Instantiate keras model\n",
    "keras.backend.clear_session()\n",
    "k_size=np.linspace(5,max_ker[0],ker[0]).astype(int)\n",
    "channels = ((-1+np.sqrt(1+4* k_size * Nparam)) / (2 *k_size)).astype(int)\n",
    "channels=channels.tolist()\n",
    "k_size=k_size.tolist()\n",
    "\n",
    "model=net.Multi_parallelKernel_CNN_modified3((nbin,1),nlayers=nlayers,channels=channels,k_size=k_size,bn=bn,nDense=nDense,nConvFinal=nConv)  \n",
    "model_name = 'Multi_parallelKernel_CNN_modified3'\n",
    "\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = []\n",
    "history = History()\n",
    "\n",
    "# Define training strategy: training is perfomed in two phases. Initially, only the reconstruction loss is optimized for epochs0 epochs, the the grad loss term is switched on and the full loss is optimized for epochs1 epochs \n",
    "\n",
    "###custom loss parameters\n",
    "Grad_weight=.6 ###relative weight between the reconstruction and grad terms of the loss\n",
    "\n",
    "\n",
    "### Global weight to avoid abrupt changes of the loss when swtiching on the grad term: to be adjusted depending on the dataset\n",
    "Weight=8.5 #High noise with L2 on grad\n",
    "\n",
    "#### For other datasets and choice of L2/L1, use:\n",
    "### Weight=8.5 #High noise L2\n",
    "### Weight=0.05 #High noise with L1 on grad\n",
    "\n",
    "\n",
    "\n",
    "### Choose between L1 and L2 norm on the grad term\n",
    "lnorm=2\n",
    "if lnorm==1:\n",
    "    custom_mse,grad_loss,mse_loss=cll.loss_def_norm1_modifiedL1(Grad_weight=Grad_weight,Weight=Weight)\n",
    "else:\n",
    "    custom_mse,grad_loss,mse_loss=cll.loss_def_norm1(Grad_weight=Grad_weight,Weight=Weight)\n",
    "\n",
    "### lr\n",
    "lr_initial=0.001\n",
    "lr_refine=0.0005\n",
    "loss0 = mse_loss\n",
    "loss1 = custom_mse\n",
    "\n",
    "### Fix the seed during hyperparameter sweeps\n",
    "fix_seed=False\n",
    "if fix_seed:\n",
    "    #Set the seed\n",
    "    from numpy.random import seed\n",
    "    seed(1)\n",
    "    tf.random.set_seed(2)\n",
    "\n",
    "    import os\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "if logger:\n",
    "# Configure Logger run (Here we use wandb)\n",
    "### Change the wandb project and entity and all the parameters accordingly\n",
    "    run = wandb.init(project='SRSdenoiser', entity='',group='', job_type='DatasetEval',config={     \n",
    "                            \"learning_rate0\": lr_initial,\n",
    "                            \"epochs0\": 25,\n",
    "                            \"batch_size0\": 32,\n",
    "                            \"loss_function0\": \"mse\",\n",
    "                            \"learning_rate1\": lr_refine,\n",
    "                            \"epochs1\": 60,\n",
    "                            \"batch_size1\": 32,\n",
    "                            \"loss_function1\" : \"custom_loss\",\n",
    "                            \"Grad_weight\" : Grad_weight,\n",
    "                            \"Weight\" : Weight,   \n",
    "                            \"architecture\": model_name,\n",
    "                            \"number_of_layers\":nlayers,\n",
    "                            \"N_denseLayers\":nDense,\n",
    "                            \"N_finalConvs\":nConv,\n",
    "                            \"Nparams\":Nparam,\n",
    "                            \"channels\":channels,\n",
    "                            \"kernel_sizes\":k_size,\n",
    "                            \"batch_norm\":bn,\n",
    "                            \"dataset\": dataset,\n",
    "                            \"lnorm\":lnorm\n",
    "                        })\n",
    "    wandb.run.name=\"Run test\"\n",
    "\n",
    "\n",
    "    ### Set up wandb log\n",
    "\n",
    "\n",
    "\n",
    "    wandb_callback = WandbCallback(monitor=\"val_loss\", verbose=0, mode=\"auto\", save_weights_only=(False),\n",
    "                    log_weights=(False), log_gradients=(False), save_model=(True),\n",
    "                    training_data=(X_train, Y_train), validation_data=None, labels=[], data_type=None,\n",
    "                    predictions=36, generator=None, input_type=None, output_type=None,\n",
    "                    log_evaluation=(False), validation_steps=None, class_colors=None,\n",
    "                    log_batch_frequency=None, log_best_prefix=\"best_\", save_graph=(True),\n",
    "                    validation_indexes=None, validation_row_processor=None,\n",
    "                    prediction_row_processor=None, infer_missing_processors=(True),\n",
    "                    log_evaluation_frequency=0)\n",
    "\n",
    "\n",
    "# Compile the model with custom loss and an adam optimizer.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_initial)\n",
    "model.compile(loss=loss0, optimizer=optimizer, metrics=[\"mae\",grad_loss,mse_loss])\n",
    "\n",
    "\n",
    "# Compile the model with custom loss and an adam optimizer.\n",
    "if logger:\n",
    "    epochs = wandb.config.epochs0\n",
    "    batch_size = wandb.config.batch_size0\n",
    "else:\n",
    "    epochs = 25\n",
    "    batch_size = 32\n",
    "\n",
    "\n",
    "### A scheduler is defined to decrease the learning rate after each epoch in a controlled way during phase 1 and 2 of the training\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 15:\n",
    "        print('----Using large lr----')\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.99#0.925\n",
    "\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_weights',\n",
    "        monitor=\"val_loss\",\n",
    "        save_weights_only=True, \n",
    "        save_best_only=True,\n",
    "        save_freq='epoch')\n",
    "\n",
    "#Define callbacks. If logger is on, also custom callback is used \n",
    "callbacks=[model_checkpoint, tf.keras.callbacks.LearningRateScheduler(scheduler)]\n",
    "\n",
    "if logger:\n",
    "    iidx=[6,283,8,991]  #Selected samples to be monitored during training\n",
    "    callbacks = [ callbacks, CustomCallback(model, X_test[iidx], Y_test[iidx], X_test, Y_test,GT_freq_test)]\n",
    "\n",
    "\n",
    "### Phase 1: Fit the model using the reconstruction loss only\n",
    "\n",
    "if logger:\n",
    "    history=model.fit(X_train, Y_train, validation_split=0.2, batch_size=batch_size, epochs=epochs, callbacks=[callbacks, history, wandb_callback])\n",
    "else:\n",
    "    history=model.fit(X_train, Y_train, validation_split=0.2, batch_size=batch_size, epochs=epochs, callbacks=[callbacks, history])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "### Phase 2: Additional training with the full loss function\n",
    "\n",
    "print('Introducing Grad Loss...')\n",
    "\n",
    "# Compile the model with custom loss and an adam optimizer.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_refine)\n",
    "model.compile(loss=loss1, optimizer=optimizer, metrics=[\"mae\",grad_loss,mse_loss])\n",
    "\n",
    "if logger:\n",
    "    epochs = wandb.config.epochs1\n",
    "    batch_size = wandb.config.batch_size1\n",
    "else:\n",
    "    epochs = 60\n",
    "    batch_size = 32\n",
    "    \n",
    "    \n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_weights',\n",
    "        monitor=\"val_loss\",\n",
    "        save_weights_only=True, \n",
    "        save_best_only=True,\n",
    "        save_freq='epoch')\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 15:\n",
    "        print('----Using large lr----')\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * .99\n",
    "\n",
    "\n",
    "callbacks=[model_checkpoint, tf.keras.callbacks.LearningRateScheduler(scheduler)]\n",
    "if logger: \n",
    "    callbacks = [callbacks,CustomCallback(model, X_test[iidx], Y_test[iidx], X_test, Y_test,GT_freq_test)]\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "\n",
    "if logger:\n",
    "    history=model.fit(X_train, Y_train, validation_split=0.2, batch_size=batch_size, epochs=epochs, callbacks=[callbacks, history,wandb_callback])\n",
    "else:\n",
    "    history=model.fit(X_train, Y_train, validation_split=0.2, batch_size=batch_size, epochs=epochs, callbacks=[callbacks, history])\n",
    "\n",
    "if logger:\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f807a0",
   "metadata": {},
   "source": [
    "## Testing and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd91e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here we show how to use the trained networks to do inference. We use samples from the test set previously unseen during training\n",
    "\n",
    "### Load raw data to be processed by the NN\n",
    "\n",
    "dataset='HighNoise'\n",
    "pathToData='Datasets/HighNoise/'\n",
    "\n",
    "\n",
    "### Use seed=1990 to split the train and test datasets consistently with what done in the paper \n",
    "normFactor, nbin, X_test,X_train,Y_test,Y_train, GT_freq, GT_freq_train, GT_freq_test = data_load(pathToData,dataset, shuf=True,seed=1990, prep=1,ShowPlot=0)\n",
    "\n",
    "### Examples from the simulated test set are already preprocessed by the data_load method\n",
    "nSample=0 ### Choose a number between 0 and 999\n",
    "Raw_data_test = X_test[nSample,:]\n",
    "GT_data_test = Y_test[nSample,:]\n",
    "\n",
    "\n",
    "### Alternatively, provide raw data formatted as an array of length=801 and store to Raw_data_test. In this case preprocessing is needed as explained below\n",
    "\n",
    "### Preprocessing of Raw data: the same preprocessing routine and NormFactor used during training must be used\n",
    "###NormFactor for the pretrained networks are provided below:\n",
    "#NormFactor = 12.4987  #NormFactor for the HighNoise dataset\n",
    "#NormFactor = 14.7908  #NormFactor for the LowNoise dataset\n",
    "\n",
    "preprocess = False  #Set to True when providing external raw data\n",
    "\n",
    "if preprocess:\n",
    "    meanX=np.mean(Raw_data_test)\n",
    "    stdX=np.std(Raw_data_test)\n",
    "    Raw_data_test-=meanX\n",
    "    Raw_data_test /= stdX\n",
    "\n",
    "    Raw_data_test /=NormFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d677f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inference phase: select a pretrained NN from the weights folder and process the raw data using the corresponding NN\n",
    "\n",
    "Raw_data_test = Raw_data_test.reshape((1,nbin,1))\n",
    "\n",
    "path_to_model = 'Weights/'\n",
    "\n",
    "model_name = 'NN_HN'\n",
    "model = tf.keras.models.load_model(path_to_model+model_name+'_model-best.h5',compile=False)\n",
    "NN_output = model.predict(Raw_data_test)\n",
    "   \n",
    "\n",
    "### Plot the results\n",
    "fig= plt.figure(figsize=(10,5))\n",
    "plt.plot(Raw_data_test.reshape(nbin,),'-r',linewidth=1,label='Raw')\n",
    "plt.plot(NN_output.reshape(nbin,),'-g',linewidth=1.5,label='NN')\n",
    "plt.plot(GT_data_test.reshape(nbin,), '--k',linewidth=1.5,label='GT')\n",
    "plt.xlabel('Absolute Raman Shift (pixels)')\n",
    "plt.ylabel('Intensity (A.U.)')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.rcParams.update({'font.size': 26})\n",
    "plt.rcParams['axes.linewidth'] = 1\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
